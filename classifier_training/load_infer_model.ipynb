{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bd33bd9",
   "metadata": {},
   "source": [
    "# 학습된 모델 가중치를 불러와 추론하기\n",
    "\n",
    "- 학습이 끝난 모델의 저장된 가중치를 불러와 학습 데이터에 존재하지 않는 새로운 입력에 대해서도 예측값을 얻어 봅시다.\n",
    "- 먼저 필요한 라이브러리들을 import 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c6efb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (4.23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5207f7d5",
   "metadata": {},
   "source": [
    "- koelectra-small 모델을 기반으로 학습한 모델의 가중치가 프로젝트 repo에 업로드되어 있습니다. 가중치 파일을 다운로드해 저장한 뒤 사용하겠습니다.\n",
    "- 본인이 직접 학습한 모델의 가중치를 사용해도 무방합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "883a6890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56564173"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download data\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "\n",
    "model_url = \"https://github.com/GirinMan/HAI-DialectTranslator/raw/main/classifier_training/model.pth\"\n",
    "model_response = requests.get(model_url, allow_redirects=True)\n",
    "\n",
    "open('./models/model.pth', 'wb').write(model_response.content)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a010014",
   "metadata": {},
   "source": [
    "- State dictionary는 모델의 종류나 구조는 저장하지 않고, 각 레이어의 가중치 값만 저장하기 때문에, 모델을 저장할 때 사용하였던 구조와 동일한 모델 객체를 불러와 사용해야 합니다.\n",
    "- 만약 학습 과정에서 다른 체크포인트를 기반으로 학습했다면, 동일하게 변경해주어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74b03f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-small-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = \"monologg/koelectra-small-v3-discriminator\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt).to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12b0e4",
   "metadata": {},
   "source": [
    "- load_state_dict 함수를 이용해 가중치를 모델에 덮어씌워줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07a36676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.load('./models/model.pth', map_location='cuda')\n",
    "model.load_state_dict(state, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e1e129",
   "metadata": {},
   "source": [
    "- 모델을 학습한 이후 테스트할 때 사용했던 코드와 유사한 infer 함수입니다.\n",
    "- tokenizer를 통해 입력을 전처리하고, 모델에 입력한 결과값을 바탕으로 표준어/방언 여부를 출력해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fc55f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, tokenizer, input_txt):\n",
    "    input_tensor = torch.tensor([tokenizer.encode(input_txt)]).to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(input_tensor).logits.cpu()\n",
    "\n",
    "    result = np.argmax(preds, axis=1).item()\n",
    "\n",
    "    print('입력된 문장 \"' + input_txt + '\"는 ', end='')\n",
    "    if result:\n",
    "        print(\"경상도 방언입니다.\")\n",
    "    else:\n",
    "        print(\"표준어 발화입니다.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac61603e",
   "metadata": {},
   "source": [
    "- 이제 사용자로부터 입력을 받아 모델을 거쳐 입력된 발화가 표준어인지 경상도 방언인지 구분해내는 프로그램을 작성해 보겠습니다.\n",
    "\n",
    "<img src=\"https://img2.quasarzone.com/editor/2021/08/20/88adb31eff6afe18af2121abc2252904.jpg\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "292e98ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "발화 텍스트를 입력하세요> 마 니 소행성이가?\n",
      "입력된 문장 \"마 니 소행성이가?\"는 경상도 방언입니다.\n",
      "발화 텍스트를 입력하세요> 나 소행성 아이다!\n",
      "입력된 문장 \"나 소행성 아이다!\"는 경상도 방언입니다.\n",
      "발화 텍스트를 입력하세요> 야 너 소행성이야?\n",
      "입력된 문장 \"야 너 소행성이야?\"는 표준어 발화입니다.\n",
      "발화 텍스트를 입력하세요> 나 소행성 아니야!\n",
      "입력된 문장 \"나 소행성 아니야!\"는 표준어 발화입니다.\n",
      "발화 텍스트를 입력하세요> \n"
     ]
    }
   ],
   "source": [
    "# 종료하려면 빈 값을 입력하세요.\n",
    "while True:\n",
    "    input_txt = input(\"발화 텍스트를 입력하세요> \")\n",
    "    if len(input_txt) == 0:\n",
    "        break\n",
    "    infer(model, tokenizer, input_txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
