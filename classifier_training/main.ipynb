{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from train import train, predict\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_loader(file_path, tokenizer, args):\n",
    "    def get_input_ids(data):\n",
    "        document_bert = [\"[CLS] \" + str(s) + \" [SEP]\" for s in data]\n",
    "        tokenized_texts = [tokenizer.tokenize(s) for s in document_bert]\n",
    "        input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "        input_ids = pad_sequences(input_ids, maxlen=args.maxlen, dtype='long', truncating='post', padding='post')\n",
    "        return input_ids\n",
    "\n",
    "    def get_attention_masks(input_ids):\n",
    "        attention_masks = []\n",
    "        for seq in input_ids:\n",
    "            seq_mask = [float(i > 0) for i in seq]\n",
    "            attention_masks.append(seq_mask)\n",
    "        return attention_masks\n",
    "\n",
    "    def get_data_loader(inputs, masks, labels, batch_size=args.batch):\n",
    "        data = TensorDataset(torch.tensor(inputs), torch.tensor(masks), torch.tensor(labels))\n",
    "        sampler = RandomSampler(data) if args.mode == 'train' else SequentialSampler(data)\n",
    "        data_loader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "        return data_loader\n",
    "\n",
    "    data_df = pd.read_csv(file_path)\n",
    "    input_ids = get_input_ids(data_df['contents'].values)\n",
    "    attention_masks = get_attention_masks(input_ids)\n",
    "    data_loader = get_data_loader(input_ids, attention_masks, data_df['label'].values if args.mode=='train' else [-1]*len(data_df))\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--train_path\", type=str, default=\"../data_preprocessing/train\")\n",
    "parser.add_argument(\"--valid_path\", type=str, default=\"../data_preprocessing/valid\")\n",
    "parser.add_argument('--device', type=str, default='cuda')\n",
    "parser.add_argument(\"--mode\", type=str, default=\"train\")\n",
    "parser.add_argument(\"--batch\", type=int, default=32)\n",
    "parser.add_argument(\"--maxlen\", type=int, default=128)\n",
    "parser.add_argument(\"--lr\", type=float, default=2e-5)\n",
    "parser.add_argument(\"--eps\", type=float, default=1e-8)\n",
    "parser.add_argument(\"--epochs\", type=int, default=4)\n",
    "parser.add_argument(\"--model_ckpt\", type=str, default=\"bert-base-multilingual-cased\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# model load\n",
    "model = AutoModelForSequenceClassification.from_pretrained(args.model_ckpt, num_labels=2)\n",
    "model.to(args.device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_ckpt)\n",
    "train_dataloader = generate_data_loader(args.train_path, tokenizer, args)\n",
    "validation_dataloader = generate_data_loader(args.valid_path, tokenizer, args)\n",
    "train(model, args, train_dataloader, validation_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
