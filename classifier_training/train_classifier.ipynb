{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import requests\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_loader(file_path, tokenizer, args):\n",
    "    def get_input_ids(data):\n",
    "        document_bert = [\"[CLS] \" + str(s) + \" [SEP]\" for s in data]\n",
    "        tokenized_texts = [tokenizer.tokenize(s) for s in document_bert]\n",
    "        input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "        input_ids = pad_sequences(input_ids, maxlen=args.maxlen, dtype='long', truncating='post', padding='post')\n",
    "        return input_ids\n",
    "\n",
    "    def get_attention_masks(input_ids):\n",
    "        attention_masks = []\n",
    "        for seq in input_ids:\n",
    "            seq_mask = [float(i > 0) for i in seq]\n",
    "            attention_masks.append(seq_mask)\n",
    "        return attention_masks\n",
    "\n",
    "    def get_data_loader(inputs, masks, labels, batch_size=args.batch):\n",
    "        data = TensorDataset(torch.tensor(inputs), torch.tensor(masks), torch.tensor(labels))\n",
    "        sampler = RandomSampler(data) if args.mode == 'train' else SequentialSampler(data)\n",
    "        data_loader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "        return data_loader\n",
    "\n",
    "    data_df = pd.read_csv(file_path)\n",
    "    input_ids = get_input_ids(data_df['contents'].values)\n",
    "    attention_masks = get_attention_masks(input_ids)\n",
    "    data_loader = get_data_loader(input_ids, attention_masks, data_df['label'].values if args.mode=='train' else [-1]*len(data_df))\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, dir_name):\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(dir_name, 'model.pth'))\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, args, data_loader):\n",
    "    print('start predict')\n",
    "    model.eval()\n",
    "\n",
    "    eval_accuracy = []\n",
    "    logits = []\n",
    "    \n",
    "    for step, batch in enumerate(data_loader):\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask)\n",
    "        logit = outputs[0]\n",
    "\n",
    "        logit = logit.detach().cpu().numpy()\n",
    "        label = b_labels.cpu().numpy()\n",
    "\n",
    "        logits.append(logit)\n",
    "\n",
    "        accuracy = flat_accuracy(logit, label)\n",
    "        eval_accuracy.append(accuracy)\n",
    "\n",
    "    logits = np.vstack(logits)\n",
    "    predict_labels = np.argmax(logits, axis=1)\n",
    "    return predict_labels, np.mean(eval_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, args, train_loader, valid_loader):\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr=args.lr,\n",
    "                      eps=args.eps\n",
    "                      )\n",
    "    total_steps = len(train_loader) * args.epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0,\n",
    "                                                num_training_steps=total_steps)\n",
    "\n",
    "    seed_val = 42\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    print('start training')\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            model.zero_grad()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "            loss = outputs[0]\n",
    "            train_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = np.mean(train_loss)\n",
    "        _, avg_val_accuracy = predict(model, args, valid_loader)\n",
    "        print(\"Epoch {0},  Average training loss: {1:.2f} , Validation accuracy : {2:.2f}\"\\\n",
    "              .format(epoch, avg_train_loss, avg_val_accuracy))\n",
    "\n",
    "        save(model, \"./saved_checkpoints/\" + str(epoch))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train_path\", type=str, default=\"./data/train\")\n",
    "    parser.add_argument(\"--valid_path\", type=str, default=\"./data/valid\")\n",
    "    parser.add_argument('--device', type=str, default='cuda')\n",
    "    parser.add_argument(\"--mode\", type=str, default=\"train\")\n",
    "    parser.add_argument(\"--batch\", type=int, default=32)\n",
    "    parser.add_argument(\"--maxlen\", type=int, default=128)\n",
    "    parser.add_argument(\"--lr\", type=float, default=2e-5)\n",
    "    parser.add_argument(\"--eps\", type=float, default=1e-8)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=4)\n",
    "    parser.add_argument(\"--model_ckpt\", type=str, default=\"bert-base-multilingual-cased\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # download data\n",
    "    os.makedirs('./data', exist_ok=True)\n",
    "    train_url = \n",
    "\n",
    "    # model load\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(args.model_ckpt, num_labels=2)\n",
    "    model.to(args.device)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_ckpt, do_lower_case=False)\n",
    "    train_dataloader = generate_data_loader(args.train_path, tokenizer, args)\n",
    "    validation_dataloader = generate_data_loader(args.valid_path, tokenizer, args)\n",
    "    train(model, args, train_dataloader, validation_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
