{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GirinMan/HAI-DialectTranslator/blob/main/multi_label_classification/train/load_infer_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bd33bd9",
      "metadata": {
        "id": "0bd33bd9"
      },
      "source": [
        "# 학습된 모델 가중치를 불러와 추론하기\n",
        "\n",
        "- 학습이 끝난 모델의 저장된 가중치를 불러와 학습 데이터에 존재하지 않는 새로운 입력에 대해서도 예측값을 얻어 봅시다.\n",
        "- 먼저 필요한 라이브러리들을 import 해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32c6efb8",
      "metadata": {
        "id": "32c6efb8",
        "outputId": "d2dbe153-2d0e-4c6a-bf8a-c0fab2be8c6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (4.23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5207f7d5",
      "metadata": {
        "id": "5207f7d5"
      },
      "source": [
        "- koelectra-small 모델을 기반으로 학습한 모델의 가중치가 프로젝트 repo에 업로드되어 있습니다. 가중치 파일을 다운로드해 저장한 뒤 사용하겠습니다.\n",
        "- 본인이 직접 학습한 모델의 가중치를 사용해도 무방합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "883a6890",
      "metadata": {
        "id": "883a6890",
        "outputId": "7eda525a-798b-4aaa-fc99-959b9d43ce6a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "113608585"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# download data\n",
        "os.makedirs('./models', exist_ok=True)\n",
        "\n",
        "model_url = \"https://github.com/GirinMan/HAI-DialectTranslator/raw/main/multi_label_classification/train/model.pth\"\n",
        "model_response = requests.get(model_url, allow_redirects=True)\n",
        "\n",
        "open('./models/model.pth', 'wb').write(model_response.content)   "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a010014",
      "metadata": {
        "id": "6a010014"
      },
      "source": [
        "- State dictionary는 모델의 종류나 구조는 저장하지 않고, 각 레이어의 가중치 값만 저장하기 때문에, 모델을 저장할 때 사용하였던 구조와 동일한 모델 객체를 불러와 사용해야 합니다.\n",
        "- 만약 학습 과정에서 다른 체크포인트를 기반으로 학습했다면, 동일하게 변경해주어야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74b03f87",
      "metadata": {
        "id": "74b03f87",
        "outputId": "af92ca51-8c93-42dc-8204-1053af744b5e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at monologg/distilkobert were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at monologg/distilkobert and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_ckpt = \"monologg/distilkobert\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=4).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc12b0e4",
      "metadata": {
        "id": "bc12b0e4"
      },
      "source": [
        "- load_state_dict 함수를 이용해 가중치를 모델에 덮어씌워줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07a36676",
      "metadata": {
        "id": "07a36676",
        "outputId": "e387f705-7fec-4327-8ae1-e9be6c5f94bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state = torch.load('./models/model.pth', map_location='cuda')\n",
        "model.load_state_dict(state, strict=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3e1e129",
      "metadata": {
        "id": "b3e1e129"
      },
      "source": [
        "- 모델을 학습한 이후 테스트할 때 사용했던 코드와 유사한 infer 함수입니다.\n",
        "- tokenizer를 통해 입력을 전처리하고, 모델에 입력한 결과값을 바탕으로 표준어/방언 여부를 출력해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fc55f62",
      "metadata": {
        "id": "1fc55f62"
      },
      "outputs": [],
      "source": [
        "def infer(model, tokenizer, input_txt):\n",
        "    input_tensor = torch.tensor([tokenizer.encode(input_txt)]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        preds = model(input_tensor).logits.cpu()\n",
        "\n",
        "    result = np.argmax(preds, axis=1).item()\n",
        "\n",
        "    region = [\"경상도\", \"제주도\", \"전라도\"]\n",
        "\n",
        "    print('입력된 문장 \"' + input_txt + '\"은/는 ', end='')\n",
        "    if result:\n",
        "        print(region[result-1], \"방언입니다.\")\n",
        "    else:\n",
        "        print(\"표준어 발화입니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac61603e",
      "metadata": {
        "id": "ac61603e"
      },
      "source": [
        "- 이제 사용자로부터 입력을 받아 모델을 거쳐 입력된 발화가 표준어인지 경상도 방언인지 구분해내는 프로그램을 작성해 보겠습니다.\n",
        "\n",
        "<img src=\"https://img2.quasarzone.com/editor/2021/08/20/88adb31eff6afe18af2121abc2252904.jpg\" width=\"300\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "292e98ce",
      "metadata": {
        "id": "292e98ce",
        "outputId": "281f093f-6c50-43c6-e17b-89c5ec406ccb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "입력된 문장 \"마 니 뭐 되나?\"은/는 전라도 방언입니다.\n",
            "입력된 문장 \"안녕하세요. 반갑습니다.\"은/는 전라도 방언입니다.\n",
            "입력된 문장 \"나랑께\"은/는 전라도 방언입니다.\n",
            "입력된 문장 \"뭐 했수광?\"은/는 전라도 방언입니다.\n"
          ]
        }
      ],
      "source": [
        "# 종료하려면 빈 값을 입력하세요.\n",
        "while True:\n",
        "    input_txt = input(\"발화 텍스트를 입력하세요> \")\n",
        "    if len(input_txt) == 0:\n",
        "        break\n",
        "    infer(model, tokenizer, input_txt)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}